# import requests
# import random
# import time

# # åŠ è½½ä»£ç†åˆ—è¡¨
# def load_proxy_list(path='./api/spider/proxies.txt'):
#     try:
#         with open(path, 'r') as f:
#             proxies = f.read().splitlines()
#             return proxies
#     except Exception as e:
#         print("âŒ åŠ è½½ä»£ç†åˆ—è¡¨å¤±è´¥:", e)
#         return []

# # å¸¦è‡ªåŠ¨åˆ‡æ¢ä»£ç†çš„è¯·æ±‚å‡½æ•°
# def request_with_retry(url, headers, cookies, max_retries=5, timeout=8):
#     proxy_list = load_proxy_list()
#     tried = set()

#     for _ in range(max_retries):
#         if len(tried) == len(proxy_list):
#             break  # å…¨éƒ¨è¯•å®Œäº†

#         proxy = random.choice(proxy_list)
#         while proxy in tried:
#             proxy = random.choice(proxy_list)
#         tried.add(proxy)

#         proxies = {
#             'http': proxy,
#             'https': proxy
#         }

#         print(f"ğŸŒ å°è¯•ä»£ç†: {proxy}")
#         try:
#             response = requests.get(url, headers=headers, cookies=cookies, proxies=proxies, timeout=timeout)
#             if response.status_code == 200:
#                 print("âœ… è¯·æ±‚æˆåŠŸ")
#                 return response
#             else:
#                 print(f"âš ï¸ çŠ¶æ€ç å¼‚å¸¸: {response.status_code}")
#         except Exception as e:
#             print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")

#         time.sleep(random.uniform(1, 2))  # é¿å…å¤ªé¢‘ç¹

#     print("ğŸš« æ‰€æœ‰ä»£ç†éƒ½å¤±è´¥äº†")
#     return None
